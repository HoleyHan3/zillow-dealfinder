{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 listings. Starting scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping listings: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def scrape_zillow_listings(url, delay=5, headers=None):\n",
    "    \"\"\"Scrapes Zillow listings for sale in a given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Zillow listings page.\n",
    "        delay (int, optional): Delay between requests in seconds. Defaults to 5.\n",
    "        headers (dict, optional): Custom headers to use for the request. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a scraped listing.\n",
    "    \"\"\"\n",
    "    \n",
    "    if headers is None:\n",
    "        # Use fake user-agent library\n",
    "        user_agent = UserAgent().random\n",
    "        headers = {\n",
    "            'User-Agent': user_agent,\n",
    "            'Accept-Language': 'en-US,en;q=0.8',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'TE': 'Trailers',\n",
    "            'DNT': '1',  # Do Not Track\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "        }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        try:\n",
    "            resp = session.get(url, headers=headers)\n",
    "            resp.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            \n",
    "            total_listings = len(soup.select(\".StyledCard-c11n-8-85-1__sc-rmiu6p-0\"))\n",
    "            print(f\"Found {total_listings} listings. Starting scraping...\")\n",
    "\n",
    "            with tqdm(total=total_listings, desc=\"Scraping listings\") as pbar:\n",
    "                listings = []\n",
    "\n",
    "                for el in soup.select(\".StyledCard-c11n-8-85-1__sc-rmiu6p-0\"):\n",
    "                    listing = {}\n",
    "\n",
    "                    try:\n",
    "                        listing[\"pricing\"] = el.find(\".bqsBln\").text.strip()\n",
    "                        listing[\"size\"] = el.find(\".gxlfal\").text.strip()\n",
    "                        listing[\"address\"] = el.find(\"address\").text.strip()\n",
    "                        listing[\"listing_by\"] = el.find(\".StyledPropertyCardDataArea-c11n-8-85-1__sc-yipmu-0.cWiizR\").text.strip()\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error extracting data: {e}\")\n",
    "\n",
    "                    listings.append(listing)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Optional delay after each listing\n",
    "                    time.sleep(delay)\n",
    "\n",
    "            return listings\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error fetching URL: {e}\")\n",
    "            return []\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.zillow.com/new-york-ny/buy/\"\n",
    "listings = scrape_zillow_listings(url)\n",
    "\n",
    "print(listings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
